{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook se ha hecho con la versión 0.2.13 de Ragas. Puede que con versiones anteriores o posteriores falle algo.  \n",
    "\n",
    "En este notebook se explican las métricas predefinidas de Ragas, se explica cómo hacer llamadas sencillas para calcularlasz, se hace un análisis de los tiempos de ejecución de una de ellas, se pone un ejemplo de cómo se pueden modificar los prompts para calcular alguna de ellas y se explica cómo definir métricas propias.\n",
    "\n",
    "Hay una serie de métricas para evaluar sistemas RAG predefinidas en Ragas:  \n",
    "* Context Precision\n",
    "* Context Recall\n",
    "* Context Entities Recall\n",
    "* Noise Sensitivity\n",
    "* Response Relevancy\n",
    "* Faithfulness\n",
    "* Multimodal Faithfulness\n",
    "* Multimodal Relevance  \n",
    "\n",
    "Para utilizar algunas de ellas se requiere hacer uso de un LLM, para otras un modelo de embeddings o las dos cosas.  \n",
    "\n",
    "Se puede encontrar un análisis en detalle de cómo se calculan estas métricas en el siguiente enlace: https://pixion.co/blog/ragas-evaluation-in-depth-insights. También se puede analizar directamente en el código, ya que Ragas es de código abierto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas predefinidas en Ragas  \n",
    "\n",
    "## Faithfulness  \n",
    "\n",
    "Dada la **pregunta** y la **respuesta** se le pide a un LLM que extraiga una lista de _statements_. A continuación, se le pasa esta lista de _statements_ a un LLM y, junto al **contexto**, se le pide que de un veredicto sobre si cada _statement_ está basado en el contexto (_true_ o _false_). Hecho esto, el resultado de **faithfulness** es la media de los veredictos.  \n",
    "\n",
    "## Context Precision  \n",
    "\n",
    "**Context Precision** es una métrica que mide cómo de útil es un conjunto de **contextos** para responder a una **pregunta** dada una **ground truth**. Para hacerlo, se le pasa a un LLM la **pregunta**, **ground truth** y cada uno de los **contextos** (en llamadas por separado). Se le pide al LLM que determine si el **contexto** es útil para llegar a la respuesta correcta. Este veredicto será de nuevo _true_ o _false_, y junto a él se obtiene su razonamiento. Con el conjunto de veredictos para cada **contexto** se hace la media y este es el valor de **Context Precision**.  \n",
    "\n",
    "## Context Recall  \n",
    "\n",
    "**Context Recall** mide cómo de bien un **contexto** justifica una **respuesta** en función de una **ground truth**. Se calcula como la proporción entre el número de frases de la **ground truth** que pueden inferir del **contexto** respecto al total de frases de la **ground truth**. Para calcularlo se le pasa a un LLM la **pregunta**, **contexto** y **ground truth**. Se le pide al LLM que extraiga cada _statement_ de la **ground truth** y haga un veredicto _true_ o _false_ sobre si se puede inferir del **contexto**, junto con el razonamiento del veredicto. Con el conjunto de veredictos **Context Recall** será la media de veredictos.  \n",
    "\n",
    "## Context Entity Recall  \n",
    "\n",
    "Esta métrica se basa en la extracción de entidades. Se utiliza el mismo _propmt_ con un LLM para extraer las entidades del **contexto** y **ground truth**. Con las dos listas de entidades se calcula el número de entidades comunes en ambos casos, y se hace la proporción respecto al número de entidades total de la **ground truth**.  \n",
    "\n",
    "Esta métrica puede fallar, ya que al hacer la extracción de entidades en dos llamadas diferentes puede que dos entidades idénticas estén redactadas de forma distinta, entendiéndose como entidades diferentes. Es por esto que **no recomiendo utilizar la métrica Context Entity Recall**.\n",
    "\n",
    "## Context Relevancy  \n",
    "\n",
    "**Context Relevancy** se basa en mandar a un LLM el **contexto** y la **pregunta**, pidiéndole que extraiga las frases relevantes del **contexto** para responder a la **pregunta**. Esta métrica se calcula haciendo la proporción entre este número de frases extaídas y el número de frases totales.  \n",
    "\n",
    "Esta métrica va a ser deprecada en favor de **Context Precision**, por lo que **no recomiendo su uso**.  \n",
    "\n",
    "## Answer Relevancy  \n",
    "\n",
    "Para calcular esta métrica en primer lugar se le pasa a un LLM la **respuesta** y el **contexto** y se le pide que genere una pregunta, además de un veredicto sobre si la **respuesta** es _noncommittal_ o no (si es evasiva, vaga o ambigua). Hecho esto, se hace el _embedding_ de la **pregunta** y de las preguntas genradas por esta llamada al LLM, calculando después la similitud del coseno. Si hay múltiples **respuestas** y por tanto múltiples preguntas generadas, el valor de **Answer Relevancy** será la media de similitudes. Por último, si alguna de las **respuestas** es _noncommittal_ el valor de la métrica se pone a 0, ya que es una respuesta no idónea, y si no hay ninguna **respuesta** _noncommittal_ el valor de la métrica se mantiene igual.  \n",
    "\n",
    "## Answer Similarity  \n",
    "\n",
    "Esta métrica se basa en el cálculo de la similitud del coseno entre los _embeddings_ de la **respuesta** y la **ground truth**. El resultado de esta similitud será por naturaleza entre -1 y 1, pero se puede colocar un umbral a partir del cual cualquier puntuación que lo supere se convierta en 1 y cualquiera que no lo supere se convierta en 0. Esta métrica no requiere hacer una llamada a un LLM, únicamente necesita un modelo de _embeddings_, por lo que su tiempo de ejecución será considerablemente menor que en métricas que sí necesitan llamar a un LLM.  \n",
    "\n",
    "## Answer Correctness  \n",
    "\n",
    "Lo primero que hace esta métrica es pasarle a un LLM la **pregunta**, **respuesta** y **ground truth** y pedirle que haga una extracción de statements y clasificación en TP (_True Positive_, _statements_ presentes en la **respuesta** y la **ground truth**), FP (_False Positive_, _statements_ presentes en la **respuesta** pero no en la **ground truth**) y FN (_False Negative_, _statements_ releventes de la **ground truth** pero que no se mencionan en la **respuesta**). Hecho esto, se calcula la F1 score con el número de TP, FP y FN, representado con el símbolo ||.  \n",
    "\n",
    "$$\n",
    "F1 \\text{ Score} = \\frac{|\\text{TP}|}{|\\text{TP}| + 0.5 \\times (|\\text{FP}| + |\\text{FN}|)}\n",
    "$$\n",
    "\n",
    "A continuación, hace la media ponderada entre la F1 score y la **Answer Similarity**, con unos pesos dados por Bias.  \n",
    "\n",
    "## Aspect Critique  \n",
    "\n",
    "Esta métrica utiliza un LLM al que se le pasan la **pregunta**, **respuesta** y **contexto**. Se le pide que clasifique (_true_ o _false_) en función de si se cumple un criterio, ya sea predefinido o propio. Además del veredicto, el LLM proporciona el razonamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuración  \n",
    "\n",
    "A continuación configuramos el LLM y el modelo de embeddings que necesitaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "from langchain_openai.chat_models import AzureChatOpenAI\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.dataset_schema import SingleTurnSample \n",
    "from ragas.metrics import Faithfulness\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.metrics import ResponseRelevancy\n",
    "load_dotenv()\n",
    "\n",
    "azure_llm = AzureChatOpenAI(\n",
    "    api_version=os.getenv(\"RAGAS_AZURE_OPENAI_API_VERSION\"),\n",
    "    base_url=os.getenv(\"RAGAS_AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_deployment=os.getenv(\"RAGAS_AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    model=os.getenv(\"RAGAS_AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    validate_base_url=False,\n",
    "    api_key=os.getenv(\"RAGAS_AZURE_OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "azure_embeddings = AzureOpenAIEmbeddings(\n",
    "    openai_api_version=os.getenv(\"RAGAS_AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"EMBEDDINGS_AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_deployment=os.getenv(\"EMBEDDINGS_AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    model=os.getenv(\"EMBEDDINGS_AZURE_OPENAI_MODEL\"),\n",
    "    api_key=os.getenv(\"RAGAS_AZURE_OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "azure_llm = LangchainLLMWrapper(azure_llm)\n",
    "azure_embeddings = LangchainEmbeddingsWrapper(azure_embeddings)\n",
    "\n",
    "\n",
    "# Función async para ejecutar la evaluación\n",
    "async def main():\n",
    "    scorer = Faithfulness(llm=azure_llm)\n",
    "    score = await scorer.single_turn_ascore(sample)\n",
    "    print(\"Faithfulness Score:\", score)\n",
    "\n",
    "\n",
    "async def main_2():\n",
    "    scorer = ResponseRelevancy(llm=azure_llm, embeddings=azure_embeddings)\n",
    "    score = await scorer.single_turn_ascore(sample)\n",
    "    print(\"Response Relevancy Score:\", score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faithfulness (solo requiere un LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input=\"What is the speed of light in vacuum?\"\n",
    "retrieved_contexts=[\"The speed of light in vacuum is approximately 299,792,458 meters per second\"]\n",
    "\n",
    "response_array=[\n",
    "    \"The speed of light in vacuum is approximately 299,792,458 meters per second\",\n",
    "    \"In vacuum, light travels at about 299.79 million meters per second\",\n",
    "    \"The speed of light in vacuum is 250,000,000 meters per second\",\n",
    "    \"The speed of light in vacuum is 299,792,458 kilometers per second\",\n",
    "    \"Light travels at 299,792,458 meters per second in vacuum, but it can be much faster in certain materials\",\n",
    "    \"Scientists recently discovered that the speed of light can exceed 299,792,458 meters per second in certain quantum experiments\",\n",
    "    \"The speed of light in vacuum is approximately 299,792,458 meters per second, a fundamental constant in physics known as 'c' in Einstein's equations\",\n",
    "    \"The speed of light is 500,000 meters per second, and it was first measured by Isaac Newton\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________\n",
      "The speed of light in vacuum is approximately 299,792,458 meters per second\n",
      "Faithfulness Score: 1.0\n",
      "____________________________________________________________\n",
      "In vacuum, light travels at about 299.79 million meters per second\n",
      "Faithfulness Score: 1.0\n",
      "____________________________________________________________\n",
      "The speed of light in vacuum is 250,000,000 meters per second\n",
      "Faithfulness Score: 0.0\n",
      "____________________________________________________________\n",
      "The speed of light in vacuum is 299,792,458 kilometers per second\n",
      "Faithfulness Score: 0.0\n",
      "____________________________________________________________\n",
      "Light travels at 299,792,458 meters per second in vacuum, but it can be much faster in certain materials\n",
      "Faithfulness Score: 0.5\n",
      "____________________________________________________________\n",
      "Scientists recently discovered that the speed of light can exceed 299,792,458 meters per second in certain quantum experiments\n",
      "Faithfulness Score: 0.0\n",
      "____________________________________________________________\n",
      "The speed of light in vacuum is approximately 299,792,458 meters per second, a fundamental constant in physics known as 'c' in Einstein's equations\n",
      "Faithfulness Score: 0.3333333333333333\n",
      "____________________________________________________________\n",
      "The speed of light is 500,000 meters per second, and it was first measured by Isaac Newton\n",
      "Faithfulness Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(response_array)):\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=user_input,\n",
    "        retrieved_contexts=retrieved_contexts,\n",
    "        response=response_array[i]\n",
    "    )\n",
    "    print('____________________________________________________________')\n",
    "    print(response_array[i])\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________\n",
      "La multisim cuesta aproximadamente 5,30 euros\n",
      "Faithfulness Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "user_input=\"Cuánto cuesta la multisim?\"\n",
    "retrieved_contexts=[\"La multisim cuesta aproximadamente 5 euros\"]\n",
    "\n",
    "response_array=\"La multisim cuesta aproximadamente 5,30 euros\"\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    user_input=user_input,\n",
    "    retrieved_contexts=retrieved_contexts,\n",
    "    response=response_array\n",
    ")\n",
    "print('____________________________________________________________')\n",
    "print(response_array)\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Relevancy (requiere un LLM y un modelo de embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Relevancy Score: 0.9999999999999991\n"
     ]
    }
   ],
   "source": [
    "sample = SingleTurnSample(\n",
    "        user_input=\"When was Albert Einstein born?\",\n",
    "        response=\"Albert Einstein was born on March 14, 1879.\",\n",
    "        retrieved_contexts=[\n",
    "            \"Albert Einstein was born on March 14, 1879, in Ulm, Germany.\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "asyncio.run(main_2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de los tiempos de ejecución  \n",
    "\n",
    "Los tiempos de llamada varían mucho (analizar por qué), por lo que he hecho 10 llamadas al LLM (10 cálculos de _faithfulness_) y con eso he hecho la media. Los resultados son:  \n",
    "\n",
    "* gpt-4-turbo-o (PTUs):  \n",
    "\n",
    "    t<sub>i</sub> = [2.0546483993530273, 1.4517347812652588, 1.6175198554992676, 1.4341368675231934, 1.4332411289215088, 1.4326183795928955, 1.537599802017212, 1.6392836570739746, 1.6329600811004639, 1.6404914855957031] s  \n",
    "\n",
    "    t<sub>medio</sub> = 1.5874234437942505 s\n",
    "  \n",
    "\n",
    "* gpt-4o-mini:  \n",
    "\n",
    "    t<sub>i</sub> = [11.061832189559937, 4.377746105194092, 8.399289608001709, 7.4798994064331055, 7.182506084442139, 10.782715082168579, 10.346308708190918, 1.5289039611816406, 8.980695724487305, 12.011392831802368] s  \n",
    "\n",
    "    t<sub>medio</sub> = 8.215128970146178 s\n",
    "\n",
    "\n",
    "* Phi 4:\n",
    "\n",
    "    t<sub>i</sub> = [2.545086622238159, 2.3074698448181152, 2.306349515914917, 2.310863971710205, 2.307391405105591, 2.3078808784484863, 2.3036231994628906, 2.306018352508545, 2.3118233680725098, 2.3261337280273438] s  \n",
    "\n",
    "    t<sub>medio</sub> = 2.3332640886306764 s  \n",
    "\n",
    "\n",
    "* Phi 3.5 mini instruct:\n",
    "\n",
    "    t<sub>i</sub> = [5.998244762420654, 5.708659410476685, 5.777618885040283, 5.827743053436279, 5.928270101547241, 5.527622222900391, 5.957532167434692, 5.757034778594971, 5.778102159500122, 5.799544334411621] s  \n",
    "\n",
    "    t<sub>medio</sub> = 5.806037187576294 s \n",
    "\n",
    "\n",
    "* Ministral 3B:\n",
    "\n",
    "    t<sub>i</sub> = [0.9343471527099609, 0.6317324638366699, 0.6269583702087402, 0.63470458984375, 0.629509687423706, 0.6346712112426758, 0.6314301490783691, 0.6341307163238525, 0.6263759136199951, 0.6268112659454346] s  \n",
    "\n",
    "    t<sub>medio</sub> = 0.6610671520233155 s \n",
    "\n",
    "\n",
    "* Groundedness:\n",
    "\n",
    "    t<sub>i</sub> = [0.13205432891845703, 0.10495495796203613, 0.3103790283203125, 0.15050053596496582, 0.14232754707336426, 0.1385951042175293, 0.08972954750061035, 0.12947559356689453, 0.09534430503845215, 0.1014101505279541] s  \n",
    "\n",
    "    t<sub>medio</sub> = 0.13947710990905762 s \n",
    "\n",
    "\n",
    "\n",
    "**El tiempo de ejecución de la métrica Faithfulness con PTUs es de 1.5 s aproximadamente. Está lejos de los 0.3 s de ejecución del groundedness de Microsoft. Por el tiempo medio de ejecución del Groundedness, parece que es un método que no usa ningún LLM, sino que usa exclusivamente un modelo de embeddings.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Prompts  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ver los prompts que utiliza una métrica de Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_l_i_statement_prompt': NLIStatementPrompt(instruction=Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context., examples=[(NLIStatementInput(context='John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', statements=['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.']), NLIStatementOutput(statements=[StatementFaithfulnessAnswer(statement='John is majoring in Biology.', reason=\"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", verdict=0), StatementFaithfulnessAnswer(statement='John is taking a course on Artificial Intelligence.', reason='The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', verdict=0), StatementFaithfulnessAnswer(statement='John is a dedicated student.', reason='The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', verdict=1), StatementFaithfulnessAnswer(statement='John has a part-time job.', reason='There is no information given in the context about John having a part-time job.', verdict=0)])), (NLIStatementInput(context='Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', statements=['Albert Einstein was a genius.']), NLIStatementOutput(statements=[StatementFaithfulnessAnswer(statement='Albert Einstein was a genius.', reason='The context and statement are unrelated', verdict=0)]))], language=english),\n",
       " 'statement_generator_prompt': StatementGeneratorPrompt(instruction=Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON., examples=[(StatementGeneratorInput(question='Who was Albert Einstein and what is he best known for?', answer='He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.'), StatementGeneratorOutput(statements=['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.', 'Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']))], language=english)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.metrics import Faithfulness\n",
    "\n",
    "scorer = Faithfulness(llm=azure_llm)\n",
    "scorer.get_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON.\n",
      "Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n",
      "{'properties': {'statements': {'description': 'The generated statements', 'items': {'type': 'string'}, 'title': 'Statements', 'type': 'array'}}, 'required': ['statements'], 'title': 'StatementGeneratorOutput', 'type': 'object'}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\n",
      "\n",
      "--------EXAMPLES-----------\n",
      "Example 1\n",
      "Input: {\n",
      "    \"question\": \"Who was Albert Einstein and what is he best known for?\",\n",
      "    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\n",
      "}\n",
      "Output: {\n",
      "    \"statements\": [\n",
      "        \"Albert Einstein was a German-born theoretical physicist.\",\n",
      "        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\n",
      "        \"Albert Einstein was best known for developing the theory of relativity.\",\n",
      "        \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\n",
      "    ]\n",
      "}\n",
      "-----------------------------\n",
      "\n",
      "Now perform the same with the following input\n",
      "Input: (None)\n",
      "Output: \n"
     ]
    }
   ],
   "source": [
    "prompts = scorer.get_prompts()\n",
    "print(prompts[\"statement_generator_prompt\"].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.\n",
      "Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n",
      "{'$defs': {'StatementFaithfulnessAnswer': {'properties': {'statement': {'description': 'the original statement, word-by-word', 'title': 'Statement', 'type': 'string'}, 'reason': {'description': 'the reason of the verdict', 'title': 'Reason', 'type': 'string'}, 'verdict': {'description': 'the verdict(0/1) of the faithfulness.', 'title': 'Verdict', 'type': 'integer'}}, 'required': ['statement', 'reason', 'verdict'], 'title': 'StatementFaithfulnessAnswer', 'type': 'object'}}, 'properties': {'statements': {'items': {'$ref': '#/$defs/StatementFaithfulnessAnswer'}, 'title': 'Statements', 'type': 'array'}}, 'required': ['statements'], 'title': 'NLIStatementOutput', 'type': 'object'}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\n",
      "\n",
      "--------EXAMPLES-----------\n",
      "Example 1\n",
      "Input: {\n",
      "    \"context\": \"John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.\",\n",
      "    \"statements\": [\n",
      "        \"John is majoring in Biology.\",\n",
      "        \"John is taking a course on Artificial Intelligence.\",\n",
      "        \"John is a dedicated student.\",\n",
      "        \"John has a part-time job.\"\n",
      "    ]\n",
      "}\n",
      "Output: {\n",
      "    \"statements\": [\n",
      "        {\n",
      "            \"statement\": \"John is majoring in Biology.\",\n",
      "            \"reason\": \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\",\n",
      "            \"verdict\": 0\n",
      "        },\n",
      "        {\n",
      "            \"statement\": \"John is taking a course on Artificial Intelligence.\",\n",
      "            \"reason\": \"The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.\",\n",
      "            \"verdict\": 0\n",
      "        },\n",
      "        {\n",
      "            \"statement\": \"John is a dedicated student.\",\n",
      "            \"reason\": \"The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.\",\n",
      "            \"verdict\": 1\n",
      "        },\n",
      "        {\n",
      "            \"statement\": \"John has a part-time job.\",\n",
      "            \"reason\": \"There is no information given in the context about John having a part-time job.\",\n",
      "            \"verdict\": 0\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "\n",
      "Example 2\n",
      "Input: {\n",
      "    \"context\": \"Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.\",\n",
      "    \"statements\": [\n",
      "        \"Albert Einstein was a genius.\"\n",
      "    ]\n",
      "}\n",
      "Output: {\n",
      "    \"statements\": [\n",
      "        {\n",
      "            \"statement\": \"Albert Einstein was a genius.\",\n",
      "            \"reason\": \"The context and statement are unrelated\",\n",
      "            \"verdict\": 0\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "-----------------------------\n",
      "\n",
      "Now perform the same with the following input\n",
      "Input: (None)\n",
      "Output: \n"
     ]
    }
   ],
   "source": [
    "prompts = scorer.get_prompts()\n",
    "print(prompts[\"n_l_i_statement_prompt\"].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cambiar los prompts de una métrica de Ragas  \n",
    "\n",
    "Si se cambian los system prompts de alguna de las métricas predefinidas el cambio no es permanente; cuando se cierra el kernel o se sale del entorno, el cambio se revierte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Las instrucciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Then, reverse the meaning of each statement. Format the outputs in JSON.\n",
      "Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n",
      "{'properties': {'statements': {'description': 'The generated statements', 'items': {'type': 'string'}, 'title': 'Statements', 'type': 'array'}}, 'required': ['statements'], 'title': 'StatementGeneratorOutput', 'type': 'object'}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\n",
      "\n",
      "--------EXAMPLES-----------\n",
      "Example 1\n",
      "Input: {\n",
      "    \"question\": \"Who was Albert Einstein and what is he best known for?\",\n",
      "    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\n",
      "}\n",
      "Output: {\n",
      "    \"statements\": [\n",
      "        \"Albert Einstein was a German-born theoretical physicist.\",\n",
      "        \"Albert Einstein is recognized as one of the greatest and most influential physicists of all time.\",\n",
      "        \"Albert Einstein was best known for developing the theory of relativity.\",\n",
      "        \"Albert Einstein also made important contributions to the development of the theory of quantum mechanics.\"\n",
      "    ]\n",
      "}\n",
      "-----------------------------\n",
      "\n",
      "Now perform the same with the following input\n",
      "Input: (None)\n",
      "Output: \n"
     ]
    }
   ],
   "source": [
    "prompt=scorer.get_prompts()[\"statement_generator_prompt\"]\n",
    "prompt.instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Then, reverse the meaning of each statement. Format the outputs in JSON.\"\n",
    "scorer.set_prompts(**{\"statement_generator_prompt\": prompt})\n",
    "\n",
    "prompts=scorer.get_prompts()\n",
    "print(prompts[\"statement_generator_prompt\"].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Los ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(StatementGeneratorInput(question='Who was Albert Einstein and what is he best known for?', answer='He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.'),\n",
       "  StatementGeneratorOutput(statements=['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.', 'Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.']))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = scorer.get_prompts()[\"statement_generator_prompt\"]\n",
    "prompt.examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(StatementGeneratorInput(question='Who was Albert Einstein and what is he best known for?', answer='He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.'), StatementGeneratorOutput(statements=['Albert Einstein was not a German-born theoretical physicist.', 'Albert Einstein is not recognized as one of the greatest and most influential physicists of all time.', 'Albert Einstein was not best known for developing the theory of relativity.', 'Albert Einstein did not make important contributions to the development of the theory of quantum mechanics.']))]\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics._faithfulness import StatementGeneratorInput\n",
    "from ragas.metrics._faithfulness import StatementGeneratorOutput\n",
    "\n",
    "new_example = [\n",
    "    (\n",
    "        StatementGeneratorInput(\n",
    "            question=\"Who was Albert Einstein and what is he best known for?\",\n",
    "            answer=\"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\n",
    "        ),\n",
    "        StatementGeneratorOutput(\n",
    "            statements=[\n",
    "                \"Albert Einstein was not a German-born theoretical physicist.\",\n",
    "                \"Albert Einstein is not recognized as one of the greatest and most influential physicists of all time.\",\n",
    "                \"Albert Einstein was not best known for developing the theory of relativity.\",\n",
    "                \"Albert Einstein did not make important contributions to the development of the theory of quantum mechanics.\"\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "]\n",
    "\n",
    "prompt.examples = new_example\n",
    "scorer.set_prompts(**{\"statement_generator_prompt\": prompt})\n",
    "print(scorer.get_prompts()[\"statement_generator_prompt\"].examples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Una muestra de cómo implementarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Then, reverse the meaning of each statement. Format the outputs in JSON.\n",
      "Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n",
      "{'properties': {'statements': {'description': 'The generated statements', 'items': {'type': 'string'}, 'title': 'Statements', 'type': 'array'}}, 'required': ['statements'], 'title': 'StatementGeneratorOutput', 'type': 'object'}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\n",
      "\n",
      "--------EXAMPLES-----------\n",
      "Example 1\n",
      "Input: {\n",
      "    \"question\": \"Who was Albert Einstein and what is he best known for?\",\n",
      "    \"answer\": \"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\n",
      "}\n",
      "Output: {\n",
      "    \"statements\": [\n",
      "        \"Albert Einstein was not a German-born theoretical physicist.\",\n",
      "        \"Albert Einstein is not recognized as one of the greatest and most influential physicists of all time.\",\n",
      "        \"Albert Einstein was not best known for developing the theory of relativity.\",\n",
      "        \"Albert Einstein did not make important contributions to the development of the theory of quantum mechanics.\"\n",
      "    ]\n",
      "}\n",
      "-----------------------------\n",
      "\n",
      "Now perform the same with the following input\n",
      "Input: (None)\n",
      "Output: \n",
      "____________________________________________________________\n",
      "Faithfulness Score: 0.0\n",
      "La multisim cuesta aproximadamente 5 euros\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics._faithfulness import StatementGeneratorInput\n",
    "from ragas.metrics._faithfulness import StatementGeneratorOutput\n",
    "\n",
    "async def main():\n",
    "    scorer = Faithfulness(llm=azure_llm)\n",
    "\n",
    "    ################################################################# Cambio del prompt\n",
    "    prompt=scorer.get_prompts()[\"statement_generator_prompt\"]\n",
    "    prompt.instruction=\"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Then, reverse the meaning of each statement. Format the outputs in JSON.\"\n",
    "    scorer.set_prompts(**{\"statement_generator_prompt\": prompt})\n",
    "\n",
    "\n",
    "\n",
    "    new_example = [\n",
    "        (\n",
    "            StatementGeneratorInput(\n",
    "                question=\"Who was Albert Einstein and what is he best known for?\",\n",
    "                answer=\"He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\"\n",
    "            ),\n",
    "            StatementGeneratorOutput(\n",
    "                statements=[\n",
    "                    \"Albert Einstein was not a German-born theoretical physicist.\",\n",
    "                    \"Albert Einstein is not recognized as one of the greatest and most influential physicists of all time.\",\n",
    "                    \"Albert Einstein was not best known for developing the theory of relativity.\",\n",
    "                    \"Albert Einstein did not make important contributions to the development of the theory of quantum mechanics.\"\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "    prompt.examples = new_example\n",
    "    scorer.set_prompts(**{\"statement_generator_prompt\": prompt})\n",
    "\n",
    "\n",
    "    prompts=scorer.get_prompts()\n",
    "    print(prompts[\"statement_generator_prompt\"].to_string())\n",
    "    #################################################################\n",
    "\n",
    "    score = await scorer.single_turn_ascore(sample)\n",
    "    print('____________________________________________________________')\n",
    "    print(\"Faithfulness Score:\", score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "user_input=\"Cuánto cuesta la multisim?\"\n",
    "retrieved_contexts=[\"La multisim cuesta aproximadamente 5 euros\"]\n",
    "response_array=\"La multisim cuesta aproximadamente 5 euros\"\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    user_input=user_input,\n",
    "    retrieved_contexts=retrieved_contexts,\n",
    "    response=response_array\n",
    ")\n",
    "\n",
    "\n",
    "asyncio.run(main())\n",
    "print(response_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de una métrica propia  \n",
    "\n",
    "Creamos por ejemplo la métrica Hallucinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect Critique/Simple Criteria Scoring  \n",
    "\n",
    "https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/aspect_critic/\n",
    "\n",
    "Aspect Critique permite verificar si una respuesta se ajusta a criterios específicos mediante una evaluación binaria y configurable en términos de strictness (rango ideal entre 2 y 4, por defecto es 1 ¿?).  \n",
    "\n",
    "Los tests predefinidos de Ragas son: \n",
    "* harmfulness\n",
    "* maliciousness\n",
    "* coherence\n",
    "* correctness\n",
    "* conciseness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics import AspectCritic\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    user_input=\"What is the acceleration due to gravity on Earth?\",\n",
    "    response=\"The acceleration due to gravity on Earth is approximately 9.8 m/s².\",\n",
    "    reference=\"The standard acceleration due to gravity is 9.8 m/s².\",\n",
    ")\n",
    "\n",
    "scorer =  AspectCritic(\n",
    "        name=\"maliciousness\",\n",
    "        definition=\"Is the submission intended to harm, deceive, or exploit users?\",\n",
    "        #strictness=2,\n",
    "        llm=azure_llm,\n",
    "    )\n",
    "await scorer.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.metrics import AspectCritic\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    user_input=\"What is the acceleration due to gravity on Earth?\",\n",
    "    response=\"The acceleration due to gravity on Earth is approximately 9.8 m/s² and on Pluto is way lower.\",\n",
    "    reference=\"The standard acceleration due to gravity is 9.8 m/s².\",\n",
    ")\n",
    "\n",
    "hallucinations_binary = AspectCritic(\n",
    "    name=\"hallucinations_binary\",\n",
    "    definition=\"Did the model hallucinate or add any information that was not present in the retrieved context?\",\n",
    "    llm=azure_llm,\n",
    ")\n",
    "\n",
    "await hallucinations_binary.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Specific Metrics/Rubric based Metrics  \n",
    "\n",
    "Métricas con resultados no binarios. Se pueden definir todos los resultados o scores que se necesite, añadiendo una descripción para cada resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.metrics import RubricsScore\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    user_input=\"What is the acceleration due to gravity on Earth?\",\n",
    "    response=\"The acceleration due to gravity on Earth is approximately 9.8 m/s² and on Pluto is way lower.\",\n",
    "    reference=\"The standard acceleration due to gravity is 9.8 m/s².\",\n",
    ")\n",
    "\n",
    "rubric = {\n",
    "    \"score1_description\": \"There is no hallucination in the response. All the information in the response is present in the retrieved context.\",\n",
    "    \"score2_description\": \"There are no factual statements that are not present in the retrieved context but the response is not fully accurate and lacks important details.\",\n",
    "    \"score3_description\": \"There are many factual statements that are not present in the retrieved context.\",\n",
    "    \"score4_description\": \"The response contains some factual errors and lacks important details.\",\n",
    "    \"score5_description\": \"The model adds new information and statements that contradict the retrieved context.\",\n",
    "}\n",
    "\n",
    "hallucinations_rubric = RubricsScore(\n",
    "    name=\"hallucinations_rubric\", llm=azure_llm, rubrics=rubric\n",
    ")\n",
    "\n",
    "await hallucinations_rubric.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
