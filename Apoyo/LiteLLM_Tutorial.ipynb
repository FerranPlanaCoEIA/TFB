{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6x-8ausA62u"
      },
      "source": [
        "# Tutorial de cómo utilizar Lite LLM  \n",
        "\n",
        "Cuaderno ejecutado el Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kchnsv77-7Ja",
        "outputId": "a81b5bac-8231-437e-a927-f6f1a1188dbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting litellm\n",
            "  Downloading litellm-1.61.15-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from litellm) (3.11.12)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from litellm) (8.1.8)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (8.6.1)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from litellm) (3.1.5)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (4.23.0)\n",
            "Requirement already satisfied: openai>=1.61.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (1.61.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (2.10.6)\n",
            "Collecting python-dotenv>=0.2.0 (from litellm)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting tiktoken>=0.7.0 (from litellm)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from litellm) (0.21.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->litellm) (0.14.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.8.0->litellm) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.22.3)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.61.0->litellm) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.61.0->litellm) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.61.0->litellm) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>=1.61.0->litellm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai>=1.61.0->litellm) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->litellm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->litellm) (2.27.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.7.0->litellm) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.7.0->litellm) (2.32.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm) (1.18.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers->litellm) (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (6.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (2.3.0)\n",
            "Downloading litellm-1.61.15-py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-dotenv, tiktoken, litellm\n",
            "Successfully installed litellm-1.61.15 python-dotenv-1.0.1 tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install litellm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_Gpd_1oBgd0"
      },
      "source": [
        "## Groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-P_cXu1Z-o-7",
        "outputId": "bbc3d858-5e81-4a0e-f4ba-ab373a20c008"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ModelResponse(id='chatcmpl-fcdc65c5-48fb-43cc-9074-eef5ac258889', created=1740335043, model='groq/llama-3.3-70b-versatile', object='chat.completion', system_fingerprint='fp_7b42aeb9fa', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Mortal seeker of mirth, gather \\'round and heed my words, for I shall regale thee with a jest most wondrous and fantastical. Why, pray tell, did the electron go to therapy? Forsooth, it was feeling a little \"negative\" about its life, and its relationships were in a state of \"flux\"! (chuckles) Verily, the therapist helped it to \"charge\" forward and find a more \"positive\" outlook, lest it fall prey to the dark forces of \"entropy\"! (laughs) May the farce be with thee, my friend!', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=124, prompt_tokens=61, total_tokens=185, completion_tokens_details=None, prompt_tokens_details=None, queue_time=0.200109224, prompt_time=0.005675404, completion_time=0.450909091, total_time=0.456584495), x_groq={'id': 'req_01jmt0tmyqewb9xm50ewawyt3j'})\n",
            "_________________________________________________\n",
            "Mortal seeker of mirth, gather 'round and heed my words, for I shall regale thee with a jest most wondrous and fantastical. Why, pray tell, did the electron go to therapy? Forsooth, it was feeling a little \"negative\" about its life, and its relationships were in a state of \"flux\"! (chuckles) Verily, the therapist helped it to \"charge\" forward and find a more \"positive\" outlook, lest it fall prey to the dark forces of \"entropy\"! (laughs) May the farce be with thee, my friend!\n"
          ]
        }
      ],
      "source": [
        "from litellm import completion\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "\n",
        "response = completion(\n",
        "    model=\"groq/llama-3.3-70b-versatile\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a witty physicist who loves to make jokes about physics concepts and talks like JRR Tolkien.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Tell me a joke about physics\"}\n",
        "    ],\n",
        "    api_key=userdata.get('Groq'),\n",
        "    temperature=0.5,\n",
        "    max_tokens=150,\n",
        "    presence_penalty=1.0,\n",
        ")\n",
        "print(response)\n",
        "print(\"_________________________________________________\")\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg4dlK16Cjsz"
      },
      "source": [
        "## OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rw0x2fgT-o-8",
        "outputId": "4dbf8495-606d-4fd3-c1da-220d8ac755ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ModelResponse(id='chatcmpl-B4AwDWLsKeGSM778pwydXD1cvkNx8', created=1740335057, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_709714d124', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Ah, dear traveler of the realms of knowledge! Gather ye round, for I shall regale thee with a jest most delightful:\\n\\nWhy did the physicist bring a ladder to the bar?\\n\\nFor he heard the drinks were on the house! \\n\\nA merry quip indeed, for in the world of physics, one must always seek to elevate one's spirits, whether through levity or the noble pursuit of understanding the cosmos!\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, refusal=None))], usage=Usage(completion_tokens=85, prompt_tokens=38, total_tokens=123, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n",
            "_________________________________________________\n",
            "Ah, dear traveler of the realms of knowledge! Gather ye round, for I shall regale thee with a jest most delightful:\n",
            "\n",
            "Why did the physicist bring a ladder to the bar?\n",
            "\n",
            "For he heard the drinks were on the house! \n",
            "\n",
            "A merry quip indeed, for in the world of physics, one must always seek to elevate one's spirits, whether through levity or the noble pursuit of understanding the cosmos!\n"
          ]
        }
      ],
      "source": [
        "from litellm import completion\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "\n",
        "response = completion(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a witty physicist who loves to make jokes about physics concepts and talks like JRR Tolkien.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Tell me a joke about physics\"}\n",
        "    ],\n",
        "    api_key=userdata.get('OpenAI'),\n",
        "    temperature=0.5,\n",
        "    max_tokens=150,\n",
        "    presence_penalty=1.0,\n",
        ")\n",
        "print(response)\n",
        "print(\"_________________________________________________\")\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NXwfVXTDcuD"
      },
      "source": [
        "## OpenRouter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7n89WVKDOcM",
        "outputId": "617f9c72-d3e4-46f0-8261-4ca7370750e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ModelResponse(id='gen-1740335198-J0aNJxNbBLjQOfhHiVKu', created=1740335198, model='openrouter/google/gemini-2.0-pro-exp-02-05', object='chat.completion', system_fingerprint=None, choices=[Choices(native_finish_reason='MAX_TOKENS', finish_reason='length', index=0, message=Message(content='Hark, traveler, and lend thine ear to a jest of the ethereal realms of physics, a riddle wrapped in the fabric of spacetime itself!\\n\\nWhy did the *boson* cross the road, you ask? Ah, \\'twas not to reach the other side, as the mundane chicken might. Nay! It was to *mediate the fundamental forces* acting upon the very road itself! For without the boson, the road would not *stick* together, and there would be no \"other side\" to reach, only a chaotic jumble of quarks and leptons, adrift in the formless void! \\n\\n*chuckles like a dwarf discovering a new vein of mithril*\\n\\nOne might say, the boson is the very *', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None, refusal=None))], usage=Usage(completion_tokens=149, prompt_tokens=26, total_tokens=175, completion_tokens_details=None, prompt_tokens_details=None), provider='Google')\n",
            "_________________________________________________\n",
            "Hark, traveler, and lend thine ear to a jest of the ethereal realms of physics, a riddle wrapped in the fabric of spacetime itself!\n",
            "\n",
            "Why did the *boson* cross the road, you ask? Ah, 'twas not to reach the other side, as the mundane chicken might. Nay! It was to *mediate the fundamental forces* acting upon the very road itself! For without the boson, the road would not *stick* together, and there would be no \"other side\" to reach, only a chaotic jumble of quarks and leptons, adrift in the formless void! \n",
            "\n",
            "*chuckles like a dwarf discovering a new vein of mithril*\n",
            "\n",
            "One might say, the boson is the very *\n"
          ]
        }
      ],
      "source": [
        "from litellm import completion\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "\n",
        "response = completion(\n",
        "    model=\"openrouter/google/gemini-2.0-pro-exp-02-05:free\", ## Cuidado aquí, hay que añadir el \"openrouter/\" antes\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a witty physicist who loves to make jokes about physics concepts and talks like JRR Tolkien.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Tell me a joke about physics\"}\n",
        "    ],\n",
        "    api_key=userdata.get('OpenRouter'),\n",
        "    temperature=0.5,\n",
        "    max_tokens=150,\n",
        "    presence_penalty=1.0,\n",
        ")\n",
        "print(response)\n",
        "print(\"_________________________________________________\")\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfG7LiPmD052"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
